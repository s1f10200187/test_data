
# AWS Glue Workflow ETLファイルダウンローダー 設計書

## 1. 概要

### 1.1 目的

AWS Glue Workflowで管理されるETLパイプラインを実行し、ソース層から最終結果層までの全ての中間処理ファイル（複数ファイル対応）を、正規表現による柔軟な条件指定でローカル環境にダウンロードするPythonツールを提供する。

### 1.2 主要機能

- **Glue Workflowの実行とモニタリング**
  - Workflow実行のトリガー
  - 実行状態のリアルタイム監視
  - 各ジョブの完了待機
- **柔軟なファイル管理**
  - YAML設定ファイルによる柔軟な設定管理
  - 各層での複数ファイル対応（正規表現による一括指定）
  - **複数の正規表現パターン対応**（1つの層で複数のファイル命名規則に対応）
  - 正規表現によるファイルフィルタリング
- **信頼性の高い処理**
  - 初期層ファイルの存在確認
  - Workflow実行完了の確認
  - 多層データレイク構造への対応（Bronze/Silver/Gold層等）
- **高速ダウンロード**
  - 並列ダウンロードによる高速化
  - エラーハンドリングと詳細なログ出力

### 1.3 対象ユーザー

- データエンジニア
- データアナリスト
- ETLパイプラインの検証・デバッグを行う開発者

-----

## 2. システムアーキテクチャ

### 2.1 全体構成図

```
┌─────────────────────────────────────────────────────────────┐
│                     ユーザー入力                              │
│  - YAML設定ファイル                                           │
│  - Glue Workflow名                                           │
│  - ダウンロード先ディレクトリ                                 │
└────────────────────┬────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│              GlueWorkflowDownloader                          │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  ConfigManager (設定管理)                            │   │
│  │  - YAML読み込み                                      │   │
│  │  - バリデーション                                    │   │
│  └─────────────────────────────────────────────────────┘   │
│                     │                                        │
│                     ▼                                        │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  WorkflowValidator (Workflow検証)                    │   │
│  │  - Workflow存在確認                                  │   │
│  │  - 初期層ファイル存在確認（複数ファイル対応）        │   │
│  └─────────────────────────────────────────────────────┘   │
│                     │                                        │
│                     ▼                                        │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  ★ WorkflowExecutor (Workflow実行制御) ★            │   │
│  │  - Workflow実行トリガー                              │   │
│  │  - 実行状態監視（ポーリング）                        │   │
│  │  - ジョブ完了待機                                    │   │
│  │  - タイムアウト処理                                  │   │
│  └─────────────────────────────────────────────────────┘   │
│                     │                                        │
│                     ▼                                        │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  S3FileCollector (ファイル収集)                      │   │
│  │  - S3バケット/パスからファイルリスト取得              │   │
│  │  - 正規表現フィルタリング（複数ファイル対応）        │   │
│  │  - 層別ファイル分類                                  │   │
│  └─────────────────────────────────────────────────────┘   │
│                     │                                        │
│                     ▼                                        │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  FileDownloader (ファイルダウンロード)                │   │
│  │  - 並列ダウンロード制御（複数ファイル対応）          │   │
│  │  - 進捗表示                                          │   │
│  │  - リトライ処理                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                     │                                        │
│                     ▼                                        │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  ReportGenerator (レポート生成)                      │   │
│  │  - ダウンロードサマリー                              │   │
│  │  - エラーレポート                                    │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────────┐
│                  AWS サービス                                 │
│  - AWS Glue (Workflow/Jobs)                                 │
│  - Amazon S3 (データレイク)                                  │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 データフロー

```
[YAML設定] → [設定検証] → [Workflow確認] → [初期層確認（複数ファイル）]
    ↓
[★ Workflow実行トリガー★]
    ↓
[★ Workflow実行監視★] → [全ジョブ完了待機]
    ↓
[各層のS3パス解決]
    ↓
[ファイルリスト取得（複数ファイル）] → [正規表現フィルタ] → [ファイル分類]
    ↓
[並列ダウンロード（複数ファイル）] → [ローカル保存] → [レポート生成]
```

-----

## 3. 詳細設計

### 3.1 モジュール構成

```
glue_workflow_downloader/
├── __init__.py
├── main.py                      # エントリーポイント
├── config/
│   ├── __init__.py
│   ├── config_manager.py        # YAML設定管理
│   └── validator.py             # 設定バリデーション
├── workflow/
│   ├── __init__.py
│   ├── workflow_manager.py      # Glue Workflow管理
│   ├── workflow_executor.py     # ★ Workflow実行制御（新規）
│   └── workflow_validator.py    # Workflow検証
├── s3/
│   ├── __init__.py
│   ├── file_collector.py        # S3ファイル収集（複数ファイル対応）
│   ├── file_matcher.py          # 正規表現マッチング
│   └── downloader.py            # ファイルダウンロード（複数ファイル対応）
├── utils/
│   ├── __init__.py
│   ├── logger.py                # ロギング
│   ├── progress.py              # 進捗表示
│   └── report.py                # レポート生成
└── cli.py                       # コマンドラインインターフェース
```

-----

## 4. YAML設定ファイル仕様

### 4.1 設定ファイル構造

```yaml
# config.yaml
version: "1.0"

# AWS接続設定
aws:
  region: ap-northeast-1
  profile: default  # オプション

# Glue Workflow設定
workflow:
  name: "etl-data-pipeline-workflow"
  execute: true  # ★ Workflowを実行するかどうか
  validate_before_run: true
  initial_layer_check_timeout: 300  # 秒
  execution_timeout: 3600  # ★ Workflow実行のタイムアウト（秒）
  polling_interval: 30  # ★ 実行状態確認の間隔（秒）
  wait_for_completion: true  # ★ 完了を待機するかどうか

# データレイク層定義
layers:
  # ソース層（Bronze層）- 複数ファイル・複数パターン対応
  - name: source
    display_name: "ソース層（生データ）"
    s3_bucket: "my-datalake-bronze"
    s3_prefix: "raw/sales/"
    file_patterns:  # ★ 複数の正規表現パターンをリストで指定
      - "^sales_\\d{8}\\.csv$"           # 日次売上データ
      - "^sales_summary_\\d{6}\\.csv$"   # 月次サマリー
      - "^sales_.*\\.json$"              # JSON形式のデータ
    required: true  # この層のファイルが必須
    min_files: 1  # 最低限必要なファイル数（全パターン合計）
    max_files: null  # 最大ファイル数（null=無制限）
    
  # 中間層1（Silver層）- 複数ファイル・複数パターン対応
  - name: staging
    display_name: "ステージング層（クレンジング済み）"
    s3_bucket: "my-datalake-silver"
    s3_prefix: "staging/sales/"
    file_patterns:
      - "^sales_cleaned_\\d{8}\\.parquet$"
      - "^sales_validated_\\d{8}\\.parquet$"
    required: false
    min_files: 0
    max_files: null
    
  # 中間層2（Silver層）- 単一パターンも引き続き対応
  - name: transformed
    display_name: "変換層（集計済み）"
    s3_bucket: "my-datalake-silver"
    s3_prefix: "transformed/sales/"
    file_patterns:
      - "^sales_aggregated_\\d{8}\\.parquet$"
    required: false
    min_files: 0
    max_files: 100  # 最大100ファイルまで
    
  # 最終結果層（Gold層）- 複数拡張子・複数パターン対応
  - name: final
    display_name: "最終結果層（分析用）"
    s3_bucket: "my-datalake-gold"
    s3_prefix: "analytics/sales/"
    file_patterns:
      - "^sales_final_\\d{8}\\.(parquet|csv)$"  # 日次最終結果
      - "^sales_monthly_\\d{6}\\.(parquet|csv)$"  # 月次集計
      - "^sales_report_.*\\.pdf$"  # レポートPDF
    required: false
    min_files: 1
    max_files: null

# ダウンロード設定
download:
  local_base_dir: "./downloads"
  preserve_structure: true  # S3のディレクトリ構造を保持
  overwrite: false  # 既存ファイルを上書きしない
  max_workers: 5  # 並列ダウンロード数
  retry_count: 3
  retry_delay: 5  # 秒
  
# ログ設定
logging:
  level: INFO  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/downloader.log"
  console: true
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# フィルタ設定（オプション）
filters:
  date_range:
    enabled: false
    start_date: "2024-01-01"
    end_date: "2024-12-31"
  file_size:
    enabled: false
    min_mb: 0
    max_mb: 1000
```

### 4.2 設定項目詳細

#### 4.2.1 AWS設定

|項目     |型     |必須|説明                        |
|-------|------|--|--------------------------|
|region |string|○ |AWSリージョン                  |
|profile|string|× |AWS CLIプロファイル名（未指定時はデフォルト）|

#### 4.2.2 Workflow設定

|項目                         |型      |必須|説明                              |
|---------------------------|-------|--|--------------------------------|
|name                       |string |○ |Glue Workflow名                  |
|execute                    |boolean|× |Workflowを実行するか（デフォルト: true）     |
|validate_before_run        |boolean|× |実行前の検証有効化（デフォルト: true）          |
|initial_layer_check_timeout|integer|× |初期層確認のタイムアウト秒数（デフォルト: 300）      |
|execution_timeout          |integer|× |Workflow実行のタイムアウト秒数（デフォルト: 3600）|
|polling_interval           |integer|× |実行状態確認の間隔秒数（デフォルト: 30）          |
|wait_for_completion        |boolean|× |完了を待機するか（デフォルト: true）           |

#### 4.2.3 層定義（layers）

|項目           |型            |必須|説明                                   |
|-------------|-------------|--|-------------------------------------|
|name         |string       |○ |層の識別子（一意）                            |
|display_name |string       |× |表示用の層名                               |
|s3_bucket    |string       |○ |S3バケット名                              |
|s3_prefix    |string       |○ |S3プレフィックス（パス）                        |
|file_patterns|array[string]|○ |★ ファイル名の正規表現パターンのリスト（複数パターン対応）       |
|required     |boolean      |× |必須層かどうか（デフォルト: false）                |
|min_files    |integer      |× |最低限必要なファイル数、全パターン合計（デフォルト: 0）        |
|max_files    |integer/null |× |最大ファイル数、全パターン合計、null=無制限（デフォルト: null）|

-----

## 5. クラス設計

### 5.1 ConfigManager クラス

**責務**: YAML設定ファイルの読み込み、検証、管理

```python
class ConfigManager:
    """
    YAML設定ファイルの管理クラス
    """
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: YAML設定ファイルのパス
        """
        self.config_path = config_path
        self.config: Dict[str, Any] = {}
        
    def load(self) -> Dict[str, Any]:
        """
        設定ファイルを読み込む
        
        Returns:
            設定辞書
            
        Raises:
            FileNotFoundError: ファイルが存在しない
            yaml.YAMLError: YAML解析エラー
            ValidationError: 設定検証エラー
        """
        pass
        
    def validate(self) -> bool:
        """
        設定内容の妥当性を検証
        
        Returns:
            検証結果
            
        Raises:
            ValidationError: 検証エラー
        """
        pass
        
    def get_layers(self) -> List[LayerConfig]:
        """
        層設定のリストを取得
        
        Returns:
            LayerConfigオブジェクトのリスト
        """
        pass
        
    def get_aws_config(self) -> Dict[str, str]:
        """AWS設定を取得"""
        pass
        
    def get_workflow_config(self) -> Dict[str, Any]:
        """Workflow設定を取得"""
        pass
        
    def get_download_config(self) -> Dict[str, Any]:
        """ダウンロード設定を取得"""
        pass
```

### 5.2 LayerConfig データクラス

```python
@dataclass
class LayerConfig:
    """データレイク層の設定（複数ファイル・複数パターン対応）"""
    name: str
    display_name: str
    s3_bucket: str
    s3_prefix: str
    file_patterns: List[str]  # ★ 複数の正規表現パターン
    required: bool = False
    min_files: int = 0  # 最低限必要なファイル数（全パターン合計）
    max_files: Optional[int] = None  # 最大ファイル数（None=無制限）
    
    def get_s3_path(self) -> str:
        """完全なS3パスを取得"""
        return f"s3://{self.s3_bucket}/{self.s3_prefix}"
    
    def matches_filename(self, filename: str) -> bool:
        """
        ファイル名がいずれかのパターンにマッチするか確認
        
        Args:
            filename: ファイル名
            
        Returns:
            いずれかのパターンにマッチする場合True
        """
        for pattern in self.file_patterns:
            if re.match(pattern, filename):
                return True
        return False
    
    def get_matched_pattern(self, filename: str) -> Optional[str]:
        """
        ファイル名にマッチしたパターンを取得
        
        Args:
            filename: ファイル名
            
        Returns:
            マッチしたパターン、マッチしない場合None
        """
        for pattern in self.file_patterns:
            if re.match(pattern, filename):
                return pattern
        return None
    
    def validate_file_count(self, file_count: int) -> bool:
        """
        ファイル数が要件を満たすか検証（全パターン合計）
        
        Args:
            file_count: 実際のファイル数
            
        Returns:
            要件を満たす場合True
        """
        if file_count < self.min_files:
            return False
        if self.max_files is not None and file_count > self.max_files:
            return False
        return True
```

### 5.3 WorkflowValidator クラス

**責務**: Glue Workflowの検証と初期層ファイルの確認

```python
class WorkflowValidator:
    """
    Glue Workflowの検証クラス
    """
    
    def __init__(self, glue_client, s3_client, config: ConfigManager):
        """
        Args:
            glue_client: boto3 Glueクライアント
            s3_client: boto3 S3クライアント
            config: ConfigManagerインスタンス
        """
        self.glue_client = glue_client
        self.s3_client = s3_client
        self.config = config
        
    def validate_workflow_exists(self, workflow_name: str) -> bool:
        """
        Workflowの存在を確認
        
        Args:
            workflow_name: Workflow名
            
        Returns:
            存在する場合True
            
        Raises:
            WorkflowNotFoundError: Workflowが存在しない
        """
        pass
        
    def check_initial_layer_files(self, timeout: int = 300) -> bool:
        """
        初期層（required=True）のファイル存在を確認（複数ファイル対応）
        
        Args:
            timeout: タイムアウト秒数
            
        Returns:
            必要なファイルが存在する場合True
            
        Raises:
            InitialLayerFileNotFoundError: ファイルが見つからない
            InsufficientFilesError: ファイル数が要件を満たさない
            TimeoutError: タイムアウト
        """
        pass
        
    def get_workflow_status(self, workflow_name: str) -> Dict[str, Any]:
        """
        Workflowの実行状態を取得
        
        Returns:
            Workflow状態情報
        """
        pass
```

### 5.4 WorkflowExecutor クラス（新規）

**責務**: Glue Workflowの実行制御と監視

```python
class WorkflowExecutor:
    """
    Glue Workflow実行制御クラス
    """
    
    def __init__(self, glue_client, config: ConfigManager):
        """
        Args:
            glue_client: boto3 Glueクライアント
            config: ConfigManagerインスタンス
        """
        self.glue_client = glue_client
        self.config = config
        
    def execute_workflow(self, workflow_name: str) -> str:
        """
        Workflowを実行
        
        Args:
            workflow_name: Workflow名
            
        Returns:
            実行ID（run_id）
            
        Raises:
            WorkflowExecutionError: 実行開始エラー
        """
        pass
        
    def wait_for_completion(
        self, 
        workflow_name: str, 
        run_id: str,
        timeout: int = 3600,
        polling_interval: int = 30
    ) -> WorkflowRunResult:
        """
        Workflow実行完了を待機
        
        Args:
            workflow_name: Workflow名
            run_id: 実行ID
            timeout: タイムアウト秒数
            polling_interval: ポーリング間隔（秒）
            
        Returns:
            実行結果
            
        Raises:
            WorkflowTimeoutError: タイムアウト
            WorkflowFailedError: Workflow実行失敗
        """
        pass
        
    def get_workflow_run_status(
        self, 
        workflow_name: str, 
        run_id: str
    ) -> Dict[str, Any]:
        """
        Workflow実行状態を取得
        
        Args:
            workflow_name: Workflow名
            run_id: 実行ID
            
        Returns:
            実行状態情報（status, completed_jobs, failed_jobs等）
        """
        pass
        
    def get_job_run_details(
        self, 
        workflow_name: str, 
        run_id: str
    ) -> List[Dict[str, Any]]:
        """
        Workflow内の各ジョブの実行詳細を取得
        
        Args:
            workflow_name: Workflow名
            run_id: 実行ID
            
        Returns:
            ジョブ実行詳細のリスト
        """
        pass
        
    def _poll_workflow_status(
        self,
        workflow_name: str,
        run_id: str,
        timeout: int,
        polling_interval: int
    ) -> str:
        """
        Workflow状態をポーリング
        
        Args:
            workflow_name: Workflow名
            run_id: 実行ID
            timeout: タイムアウト秒数
            polling_interval: ポーリング間隔
            
        Returns:
            最終状態（COMPLETED, FAILED, STOPPED等）
        """
        pass
```

### 5.5 WorkflowRunResult データクラス

```python
@dataclass
class WorkflowRunResult:
    """Workflow実行結果"""
    run_id: str
    workflow_name: str
    status: str  # COMPLETED, FAILED, STOPPED, RUNNING
    start_time: datetime
    end_time: Optional[datetime]
    duration_seconds: Optional[float]
    completed_jobs: int
    failed_jobs: int
    total_jobs: int
    job_details: List[Dict[str, Any]]
    error_message: Optional[str] = None
    
    def is_successful(self) -> bool:
        """実行が成功したか"""
        return self.status == "COMPLETED" and self.failed_jobs == 0
    
    def get_success_rate(self) -> float:
        """ジョブ成功率を取得"""
        if self.total_jobs == 0:
            return 0.0
        return (self.completed_jobs / self.total_jobs) * 100
```

### 5.6 S3FileCollector クラス

**責務**: S3からのファイル情報収集とフィルタリング（複数ファイル・複数パターン対応）

```python
class S3FileCollector:
    """
    S3ファイル収集クラス（複数ファイル・複数パターン対応）
    """
    
    def __init__(self, s3_client, config: ConfigManager):
        """
        Args:
            s3_client: boto3 S3クライアント
            config: ConfigManagerインスタンス
        """
        self.s3_client = s3_client
        self.config = config
        self.file_matcher = FileMatcher()
        
    def collect_files_for_layer(
        self, 
        layer: LayerConfig
    ) -> List[S3FileInfo]:
        """
        指定層のファイル情報を収集（複数ファイル・複数パターン対応）
        
        Args:
            layer: 層設定（複数の正規表現パターンを含む）
            
        Returns:
            S3ファイル情報のリスト（全パターンにマッチするファイル）
        """
        pass
        
    def collect_all_layers(self) -> Dict[str, List[S3FileInfo]]:
        """
        全層のファイル情報を収集（複数ファイル・複数パターン対応）
        
        Returns:
            層名をキーとしたファイル情報辞書
        """
        pass
        
    def _list_s3_objects(
        self, 
        bucket: str, 
        prefix: str
    ) -> List[Dict[str, Any]]:
        """
        S3オブジェクトをリスト
        
        Args:
            bucket: バケット名
            prefix: プレフィックス
            
        Returns:
            S3オブジェクトのリスト
        """
        pass
```

### 5.7 S3FileInfo データクラス

```python
@dataclass
class S3FileInfo:
    """S3ファイル情報（複数パターン対応）"""
    bucket: str
    key: str
    size: int
    last_modified: datetime
    layer_name: str
    matched_pattern: Optional[str] = None  # ★ マッチした正規表現パターン
    
    def get_s3_uri(self) -> str:
        """S3 URIを取得"""
        return f"s3://{self.bucket}/{self.key}"
    
    def get_filename(self) -> str:
        """ファイル名を取得"""
        return os.path.basename(self.key)
    
    def get_size_mb(self) -> float:
        """ファイルサイズ（MB）を取得"""
        return self.size / (1024 * 1024)
```

### 5.8 FileMatcher クラス

**責務**: 正規表現によるファイルマッチング（複数パターン対応）

```python
class FileMatcher:
    """
    ファイル名の正規表現マッチングクラス（複数パターン対応）
    """
    
    def __init__(self):
        self._pattern_cache: Dict[str, re.Pattern] = {}
        
    def matches(self, filename: str, pattern: str) -> bool:
        """
        ファイル名がパターンにマッチするか確認
        
        Args:
            filename: ファイル名
            pattern: 正規表現パターン
            
        Returns:
            マッチする場合True
        """
        pass
    
    def matches_any(self, filename: str, patterns: List[str]) -> bool:
        """
        ファイル名がいずれかのパターンにマッチするか確認（複数パターン対応）
        
        Args:
            filename: ファイル名
            patterns: 正規表現パターンのリスト
            
        Returns:
            いずれかのパターンにマッチする場合True
        """
        pass
    
    def get_matched_pattern(
        self, 
        filename: str, 
        patterns: List[str]
    ) -> Optional[str]:
        """
        ファイル名にマッチしたパターンを取得（複数パターン対応）
        
        Args:
            filename: ファイル名
            patterns: 正規表現パターンのリスト
            
        Returns:
            マッチしたパターン、マッチしない場合None
        """
        pass
        
    def filter_files(
        self, 
        files: List[str], 
        pattern: str
    ) -> List[str]:
        """
        ファイルリストをパターンでフィルタ
        
        Args:
            files: ファイル名のリスト
            pattern: 正規表現パターン
            
        Returns:
            マッチしたファイルのリスト
        """
        pass
    
    def filter_files_multi_pattern(
        self,
        files: List[str],
        patterns: List[str]
    ) -> Dict[str, List[str]]:
        """
        ファイルリストを複数パターンでフィルタ（複数パターン対応）
        
        Args:
            files: ファイル名のリスト
            patterns: 正規表現パターンのリスト
            
        Returns:
            パターンをキーとしたファイルリストの辞書
        """
        pass
        
    def _get_compiled_pattern(self, pattern: str) -> re.Pattern:
        """コンパイル済みパターンを取得（キャッシュ使用）"""
        pass
```

### 5.9 FileDownloader クラス

**責務**: S3からのファイルダウンロード

```python
class FileDownloader:
    """
    S3ファイルダウンローダークラス
    """
    
    def __init__(
        self, 
        s3_client, 
        config: ConfigManager,
        progress_tracker: ProgressTracker
    ):
        """
        Args:
            s3_client: boto3 S3クライアント
            config: ConfigManagerインスタンス
            progress_tracker: 進捗トラッカー
        """
        self.s3_client = s3_client
        self.config = config
        self.progress_tracker = progress_tracker
        
    def download_files(
        self, 
        files: Dict[str, List[S3FileInfo]]
    ) -> DownloadResult:
        """
        ファイルをダウンロード
        
        Args:
            files: 層名をキーとしたファイル情報辞書
            
        Returns:
            ダウンロード結果
        """
        pass
        
    def _download_single_file(
        self, 
        file_info: S3FileInfo, 
        local_path: str
    ) -> bool:
        """
        単一ファイルをダウンロード
        
        Args:
            file_info: S3ファイル情報
            local_path: ローカル保存パス
            
        Returns:
            成功した場合True
        """
        pass
        
    def _get_local_path(
        self, 
        file_info: S3FileInfo
    ) -> str:
        """
        ローカル保存パスを取得
        
        Args:
            file_info: S3ファイル情報
            
        Returns:
            ローカルパス
        """
        pass
        
    def _should_download(
        self, 
        file_info: S3FileInfo, 
        local_path: str
    ) -> bool:
        """
        ダウンロードが必要か判定
        
        Args:
            file_info: S3ファイル情報
            local_path: ローカルパス
            
        Returns:
            ダウンロードが必要な場合True
        """
        pass
```

### 5.10 DownloadResult データクラス

```python
@dataclass
class DownloadResult:
    """ダウンロード結果"""
    total_files: int
    successful: int
    failed: int
    skipped: int
    total_size_mb: float
    duration_seconds: float
    failed_files: List[Tuple[S3FileInfo, str]]  # (ファイル情報, エラーメッセージ)
    
    def get_success_rate(self) -> float:
        """成功率を取得"""
        if self.total_files == 0:
            return 0.0
        return (self.successful / self.total_files) * 100
```

### 5.11 GlueWorkflowDownloader クラス（メインクラス）

**責務**: 全体の処理フロー制御

```python
class GlueWorkflowDownloader:
    """
    Glue Workflow ETLファイルダウンローダーのメインクラス
    """
    
    def __init__(self, config_path: str):
        """
        Args:
            config_path: YAML設定ファイルのパス
        """
        self.config = ConfigManager(config_path)
        self.config.load()
        
        # AWSクライアント初期化
        aws_config = self.config.get_aws_config()
        self.session = self._create_session(aws_config)
        self.glue_client = self.session.client('glue')
        self.s3_client = self.session.client('s3')
        
        # 各コンポーネント初期化
        self.workflow_validator = WorkflowValidator(
            self.glue_client, 
            self.s3_client, 
            self.config
        )
        self.workflow_executor = WorkflowExecutor(  # ★ 新規追加
            self.glue_client,
            self.config
        )
        self.file_collector = S3FileCollector(
            self.s3_client, 
            self.config
        )
        self.progress_tracker = ProgressTracker()
        self.downloader = FileDownloader(
            self.s3_client, 
            self.config, 
            self.progress_tracker
        )
        self.report_generator = ReportGenerator()
        
    def run(self, workflow_name: str) -> DownloadResult:
        """
        ダウンロード処理を実行（Workflow実行 → ファイルダウンロード）
        
        Args:
            workflow_name: Glue Workflow名
            
        Returns:
            ダウンロード結果
            
        Raises:
            WorkflowValidationError: Workflow検証エラー
            WorkflowExecutionError: Workflow実行エラー
            DownloadError: ダウンロードエラー
        """
        logger.info(f"ダウンロード処理を開始: workflow={workflow_name}")
        
        # 1. Workflow検証
        self._validate_workflow(workflow_name)
        
        # 2. 初期層ファイル確認（複数ファイル対応）
        self._check_initial_layer()
        
        # ★ 3. Workflow実行（新規追加）
        workflow_result = self._execute_workflow(workflow_name)
        
        # ★ 4. ファイル収集（実行後に生成されたファイル、複数ファイル対応）
        files = self._collect_files()
        
        # ★ 5. ダウンロード実行（複数ファイル対応）
        result = self._download_files(files)
        
        # ★ 6. レポート生成（Workflow実行結果も含む）
        self._generate_report(result, workflow_result)
        
        logger.info("ダウンロード処理が完了しました")
        return result
        
    def _validate_workflow(self, workflow_name: str) -> None:
        """Workflowを検証"""
        pass
        
    def _check_initial_layer(self) -> None:
        """初期層ファイルを確認（複数ファイル対応）"""
        pass
        
    def _execute_workflow(self, workflow_name: str) -> WorkflowRunResult:
        """
        Workflowを実行し完了を待機（新規追加）
        
        Args:
            workflow_name: Workflow名
            
        Returns:
            Workflow実行結果
        """
        pass
        
    def _collect_files(self) -> Dict[str, List[S3FileInfo]]:
        """ファイルを収集（複数ファイル対応）"""
        pass
        
    def _download_files(
        self, 
        files: Dict[str, List[S3FileInfo]]
    ) -> DownloadResult:
        """ファイルをダウンロード"""
        pass
        
    def _generate_report(
        self, 
        result: DownloadResult,
        workflow_result: WorkflowRunResult
    ) -> None:
        """レポートを生成（Workflow実行結果を含む）"""
        pass
        
    def _create_session(
        self, 
        aws_config: Dict[str, str]
    ) -> boto3.Session:
        """AWS Sessionを作成"""
        pass
```

-----

## 6. エラーハンドリング

### 6.1 カスタム例外クラス

```python
class GlueWorkflowDownloaderError(Exception):
    """基底例外クラス"""
    pass

class ConfigurationError(GlueWorkflowDownloaderError):
    """設定エラー"""
    pass

class ValidationError(GlueWorkflowDownloaderError):
    """検証エラー"""
    pass

class WorkflowNotFoundError(GlueWorkflowDownloaderError):
    """Workflow未検出エラー"""
    pass

class InitialLayerFileNotFoundError(GlueWorkflowDownloaderError):
    """初期層ファイル未検出エラー"""
    pass

class InsufficientFilesError(GlueWorkflowDownloaderError):
    """★ ファイル数不足エラー（新規追加）"""
    pass

class TooManyFilesError(GlueWorkflowDownloaderError):
    """★ ファイル数超過エラー（新規追加）"""
    pass

class WorkflowExecutionError(GlueWorkflowDownloaderError):
    """★ Workflow実行エラー（新規追加）"""
    pass

class WorkflowTimeoutError(GlueWorkflowDownloaderError):
    """★ Workflowタイムアウトエラー（新規追加）"""
    pass

class WorkflowFailedError(GlueWorkflowDownloaderError):
    """★ Workflow実行失敗エラー（新規追加）"""
    pass

class DownloadError(GlueWorkflowDownloaderError):
    """ダウンロードエラー"""
    pass

class S3AccessError(GlueWorkflowDownloaderError):
    """S3アクセスエラー"""
    pass
```

### 6.2 エラーハンドリング戦略

|エラー種別           |処理方針     |リトライ|ログレベル  |
|----------------|---------|----|-------|
|設定ファイルエラー       |即座に終了    |なし  |ERROR  |
|Workflow未検出     |即座に終了    |なし  |ERROR  |
|初期層ファイル未検出      |タイムアウト後終了|あり  |ERROR  |
|ファイル数不足/超過      |即座に終了    |なし  |ERROR  |
|★ Workflow実行エラー |即座に終了    |なし  |ERROR  |
|★ Workflowタイムアウト|即座に終了    |なし  |ERROR  |
|★ Workflow実行失敗  |即座に終了    |なし  |ERROR  |
|S3アクセスエラー       |リトライ後スキップ|あり  |WARNING|
|ダウンロードエラー       |リトライ後スキップ|あり  |WARNING|
|一部ダウンロード失敗      |処理継続     |-   |WARNING|

-----

## 7. 処理フロー

### 7.1 メイン処理フロー

```
[開始]
  │
  ├─[1. 設定ファイル読み込み]
  │   ├─ YAML解析
  │   ├─ 設定検証
  │   └─ AWS認証情報設定
  │
  ├─[2. Workflow検証]
  │   ├─ Workflow存在確認
  │   └─ Workflow状態取得
  │
  ├─[3. 初期層ファイル確認（複数ファイル対応）]
  │   ├─ required=true の層を特定
  │   ├─ S3でファイル存在確認
  │   ├─ ファイル数が min_files 以上か確認
  │   └─ タイムアウト処理
  │
  ├─[★ 4. Workflow実行（新規）]
  │   ├─ Workflow実行トリガー
  │   ├─ 実行ID取得
  │   └─ 実行開始確認
  │
  ├─[★ 5. Workflow完了待機（新規）]
  │   ├─ 実行状態をポーリング
  │   ├─ 各ジョブの状態監視
  │   ├─ タイムアウト処理
  │   └─ 実行結果取得
  │
  ├─[6. ファイル収集（複数ファイル対応）]
  │   ├─ 各層のS3パスを走査
  │   ├─ ファイルリスト取得（複数ファイル）
  │   ├─ 正規表現フィルタ適用
  │   ├─ ファイル数検証（min_files/max_files）
  │   └─ ファイル情報を収集
  │
  ├─[7. ダウンロード実行（複数ファイル対応）]
  │   ├─ ローカルディレクトリ作成
  │   ├─ 並列ダウンロード
  │   │   ├─ 既存ファイルチェック
  │   │   ├─ ダウンロード実行（複数ファイル）
  │   │   ├─ リトライ処理
  │   │   └─ 進捗更新
  │   └─ 結果集計
  │
  ├─[8. レポート生成]
  │   ├─ Workflow実行結果サマリー
  │   ├─ ダウンロードサマリー
  │   ├─ エラーリスト作成
  │   └─ レポート出力
  │
[終了]
```

### 7.2 初期層ファイル確認フロー

```
[初期層確認開始（複数ファイル対応）]
  │
  ├─[required=trueの層を抽出]
  │
  ├─[各層についてループ]
  │   │
  │   ├─[S3バケット/プレフィックスを取得]
  │   │
  │   ├─[ファイルリストを取得（複数ファイル）]
  │   │
  │   ├─[正規表現でフィルタ]
  │   │
  │   ├─[マッチしたファイル数をカウント]
  │   │
  │   ├─[ファイル数が要件を満たすか確認]
  │   │   ├─ file_count >= min_files かつ
  │   │   └─ file_count <= max_files (max_files != null の場合)
  │   │
  │   └─[ファイルが存在し要件を満たすか確認]
  │       ├─ YES: 次の層へ
  │       └─ NO: リトライ待機
  │           ├─ タイムアウト内: 再確認
  │           └─ タイムアウト: エラー
  │               ├─ ファイル数不足 → InsufficientFilesError
  │               ├─ ファイル数超過 → TooManyFilesError
  │               └─ ファイル未検出 → InitialLayerFileNotFoundError
  │
  ├─[全ての必須層でファイル確認完了]
  │
[初期層確認完了]
```

### 7.3 Workflow実行フロー（新規）

```
[★ Workflow実行開始★]
  │
  ├─[Workflow実行をトリガー]
  │   ├─ start_workflow_run API呼び出し
  │   └─ 実行ID（run_id）取得
  │
  ├─[実行状態をポーリング]
  │   │
  │   ├─[ポーリングループ]
  │   │   │
  │   │   ├─[get_workflow_run API呼び出し]
  │   │   │
  │   │   ├─[実行状態を確認]
  │   │   │   ├─ RUNNING: 待機してポーリング継続
  │   │   │   ├─ COMPLETED: 成功完了
  │   │   │   ├─ FAILED: 失敗終了
  │   │   │   └─ STOPPED: 停止終了
  │   │   │
  │   │   ├─[各ジョブの状態を確認]
  │   │   │   ├─ 完了ジョブ数
  │   │   │   ├─ 実行中ジョブ数
  │   │   │   └─ 失敗ジョブ数
  │   │   │
  │   │   ├─[進捗をログ出力]
  │   │   │
  │   │   └─[polling_interval秒待機]
  │   │
  │   └─[タイムアウトチェック]
  │       ├─ execution_timeout内: ポーリング継続
  │       └─ タイムアウト: WorkflowTimeoutError
  │
  ├─[実行結果を取得]
  │   ├─ 実行時間
  │   ├─ 完了ジョブ数
  │   ├─ 失敗ジョブ数
  │   └─ エラーメッセージ（失敗時）
  │
  ├─[実行結果を検証]
  │   ├─ status == COMPLETED かつ failed_jobs == 0: 成功
  │   └─ それ以外: WorkflowFailedError
  │
[Workflow実行完了]
```

### 7.4 並列ダウンロードフロー（複数ファイル対応）

```
[ダウンロード開始（複数ファイル対応）]
  │
  ├─[ThreadPoolExecutor初期化]
  │   └─ max_workers数のスレッド作成
  │
  ├─[各ファイルについて（複数ファイルをループ）]
  │   │
  │   ├─[ローカルパス決定]
  │   │   ├─ preserve_structure=true: S3構造を保持
  │   │   └─ preserve_structure=false: フラットに保存
  │   │
  │   ├─[既存ファイルチェック]
  │   │   ├─ overwrite=false かつ 存在: スキップ
  │   │   └─ それ以外: ダウンロード
  │   │
  │   ├─[ダウンロード実行]
  │   │   ├─ S3からファイル取得
  │   │   ├─ ローカルに保存
  │   │   └─ 進捗更新
  │   │
  │   └─[エラー時]
  │       ├─ retry_count内: リトライ
  │       └─ retry_count超過: 失敗記録
  │
  ├─[全ファイル処理完了待機（複数ファイル）]
  │
  ├─[結果集計]
  │   ├─ 成功数
  │   ├─ 失敗数
  │   ├─ スキップ数
  │   ├─ 合計ファイル数
  │   └─ 合計サイズ
  │
[ダウンロード完了]
```

-----

## 8. CLI インターフェース

### 8.1 コマンド仕様

```bash
# 基本的な使用方法
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow etl-data-pipeline-workflow \
    --output ./downloads

# 詳細オプション
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow etl-data-pipeline-workflow \
    --output ./downloads \
    --log-level DEBUG \
    --max-workers 10 \
    --overwrite \
    --dry-run
```

### 8.2 コマンドライン引数

|引数                |短縮|必須|説明                             |デフォルト      |
|------------------|--|--|-------------------------------|-----------|
|–config           |-c|○ |YAML設定ファイルパス                   |-          |
|–workflow         |-w|○ |Glue Workflow名                 |-          |
|–output           |-o|× |ダウンロード先ディレクトリ                  |./downloads|
|–no-execute       |- |× |★ Workflowを実行せずダウンロードのみ        |false      |
|–execution-timeout|- |× |★ Workflow実行タイムアウト（秒）          |3600       |
|–log-level        |-l|× |ログレベル（DEBUG/INFO/WARNING/ERROR）|INFO       |
|–max-workers      |-m|× |並列ダウンロード数                      |5          |
|–overwrite        |- |× |既存ファイルを上書き                     |false      |
|–dry-run          |-n|× |実行せずに確認のみ                      |false      |
|–skip-validation  |- |× |Workflow検証をスキップ                |false      |
|–help             |-h|× |ヘルプ表示                          |-          |
|–version          |-v|× |バージョン表示                        |-          |

### 8.3 実行例

```bash
# 例1: 基本的な実行（Workflowを実行してダウンロード）
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow

# 例2: Workflowを実行せずダウンロードのみ
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow \
    --no-execute

# 例3: ドライラン（Workflow実行とダウンロードの確認のみ）
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow \
    --dry-run

# 例4: デバッグモードで詳細ログ出力
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow \
    --log-level DEBUG \
    --output /data/downloads

# 例5: 既存ファイルを上書きして高速ダウンロード
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow \
    --overwrite \
    --max-workers 20

# 例6: Workflow実行タイムアウトを指定
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow \
    --execution-timeout 7200
```

-----

## 9. ログ出力仕様

### 9.1 ログレベル定義

|レベル    |用途    |出力内容                   |
|-------|------|-----------------------|
|DEBUG  |詳細デバッグ|全ての処理ステップ、変数値、API呼び出し詳細|
|INFO   |通常処理  |処理開始/終了、進捗、成功メッセージ     |
|WARNING|警告    |スキップされたファイル、リトライ、一部失敗  |
|ERROR  |エラー   |処理失敗、例外発生              |

### 9.2 ログフォーマット

```
2024-11-14 10:30:45,123 - glue_workflow_downloader.main - INFO - ダウンロード処理を開始: workflow=etl-pipeline
2024-11-14 10:30:46,234 - glue_workflow_downloader.workflow - INFO - Workflow検証成功: etl-pipeline
2024-11-14 10:30:47,345 - glue_workflow_downloader.s3 - INFO - ソース層のファイル検出: 12件
2024-11-14 10:30:48,456 - glue_workflow_downloader.s3 - INFO - ステージング層のファイル検出: 12件
2024-11-14 10:30:49,567 - glue_workflow_downloader.downloader - INFO - ダウンロード開始: 合計24ファイル (156.3 MB)
2024-11-14 10:31:05,678 - glue_workflow_downloader.downloader - INFO - ダウンロード完了: 成功24件, 失敗0件 (15.2秒)
```

### 9.3 主要ログメッセージ

#### 正常処理

```
[INFO] 設定ファイル読み込み成功: config.yaml
[INFO] Workflow検証成功: {workflow_name}
[INFO] 初期層ファイル確認完了: {layer_name} ({file_count}件)
[INFO] ★ Workflow実行開始: {workflow_name} (run_id: {run_id})
[INFO] ★ Workflow実行中: 完了ジョブ {completed}/{total}, 失敗ジョブ {failed}
[INFO] ★ Workflow実行完了: {workflow_name} (所要時間: {duration}秒)
[INFO] ファイル収集完了: 合計{total_files}ファイル ({total_size_mb} MB)
[INFO] 各層のファイル数: {layer_name}={file_count}件
[INFO] ダウンロード開始: {file_count}ファイル
[INFO] ダウンロード完了: 成功{success}件, 失敗{failed}件, スキップ{skipped}件
[INFO] レポート生成完了: {report_path}
```

#### 警告

```
[WARNING] ファイルが既に存在するためスキップ: {local_path}
[WARNING] ダウンロードリトライ: {s3_uri} (試行{retry_count}/{max_retry})
[WARNING] 正規表現にマッチするファイルが見つかりません: {layer_name}
[WARNING] ★ Workflow実行中に一部ジョブが失敗: {failed_job_names}
[WARNING] ★ ファイル数が推奨範囲外: {layer_name} ({file_count}件, 推奨: {min_files}-{max_files}件)
```

#### エラー

```
[ERROR] 設定ファイルが見つかりません: {config_path}
[ERROR] Workflowが存在しません: {workflow_name}
[ERROR] 初期層ファイルが見つかりません: {layer_name} (タイムアウト: {timeout}秒)
[ERROR] ★ ファイル数不足: {layer_name} (検出: {file_count}件, 最低: {min_files}件)
[ERROR] ★ ファイル数超過: {layer_name} (検出: {file_count}件, 最大: {max_files}件)
[ERROR] ★ Workflow実行エラー: {workflow_name} - {error_message}
[ERROR] ★ Workflow実行タイムアウト: {workflow_name} (タイムアウト: {timeout}秒)
[ERROR] ★ Workflow実行失敗: {workflow_name} - 失敗ジョブ {failed_jobs}件
[ERROR] ダウンロード失敗: {s3_uri} - {error_message}
[ERROR] S3アクセスエラー: {bucket}/{key} - {error_message}
```

-----

## 10. レポート出力仕様

### 10.1 レポートファイル構成

```
downloads/
├── report_20241114_103045.txt       # テキストレポート
├── report_20241114_103045.json      # JSON形式レポート
└── layers/
    ├── source/
    │   └── sales_20241114.csv
    ├── staging/
    │   └── sales_cleaned_20241114.parquet
    └── final/
        └── sales_final_20241114.parquet
```

### 10.2 テキストレポート形式

```
================================================================================
Glue Workflow ETL ファイルダウンロード レポート
================================================================================

実行日時: 2024-11-14 10:30:45
Workflow名: etl-data-pipeline-workflow
設定ファイル: config.yaml

--------------------------------------------------------------------------------
★ Workflow実行結果 ★
--------------------------------------------------------------------------------
実行ID: wfr-12345678-abcd-efgh-ijkl-1234567890ab
実行状態: COMPLETED
開始時刻: 2024-11-14 10:30:50
終了時刻: 2024-11-14 10:45:20
実行時間: 870 秒 (14分30秒)
総ジョブ数: 8
完了ジョブ数: 8
失敗ジョブ数: 0
成功率: 100.0%

--------------------------------------------------------------------------------
ダウンロードサマリー
--------------------------------------------------------------------------------
総ファイル数: 48 (複数ファイル対応)
成功: 48 (100.0%)
失敗: 0 (0.0%)
スキップ: 0 (0.0%)
合計サイズ: 312.8 MB
処理時間: 25.3 秒
平均速度: 12.4 MB/秒

--------------------------------------------------------------------------------
層別ダウンロード状況（複数ファイル対応）
--------------------------------------------------------------------------------

[ソース層（生データ）]
  バケット: my-datalake-bronze
  プレフィックス: raw/sales/
  ファイル数: 12 (要件: 最低1件、最大無制限)
  合計サイズ: 78.5 MB
  状態: 成功

[ステージング層（クレンジング済み）]
  バケット: my-datalake-silver
  プレフィックス: staging/sales/
  ファイル数: 12 (要件: 最低0件、最大無制限)
  合計サイズ: 77.8 MB
  状態: 成功

[変換層（集計済み）]
  バケット: my-datalake-silver
  プレフィックス: transformed/sales/
  ファイル数: 12 (要件: 最低0件、最大100件)
  合計サイズ: 78.2 MB
  状態: 成功

[最終結果層（分析用）]
  バケット: my-datalake-gold
  プレフィックス: analytics/sales/
  ファイル数: 12 (要件: 最低1件、最大無制限)
  合計サイズ: 78.3 MB
  状態: 成功

--------------------------------------------------------------------------------
ダウンロード済みファイルリスト（複数ファイル）
--------------------------------------------------------------------------------

ソース層 (12ファイル):
  1. sales_20241101.csv (6.5 MB) -> downloads/layers/source/sales_20241101.csv
  2. sales_20241102.csv (6.6 MB) -> downloads/layers/source/sales_20241102.csv
  3. sales_20241103.csv (6.4 MB) -> downloads/layers/source/sales_20241103.csv
  ...

ステージング層 (12ファイル):
  1. sales_cleaned_20241101.parquet (6.4 MB) -> downloads/layers/staging/...
  2. sales_cleaned_20241102.parquet (6.5 MB) -> downloads/layers/staging/...
  ...

--------------------------------------------------------------------------------
エラー/警告
--------------------------------------------------------------------------------
なし

================================================================================
```

### 10.3 JSONレポート形式

```json
{
  "execution_info": {
    "timestamp": "2024-11-14T10:30:45+09:00",
    "workflow_name": "etl-data-pipeline-workflow",
    "config_file": "config.yaml",
    "output_directory": "./downloads"
  },
  "workflow_execution": {
    "run_id": "wfr-12345678-abcd-efgh-ijkl-1234567890ab",
    "status": "COMPLETED",
    "start_time": "2024-11-14T10:30:50+09:00",
    "end_time": "2024-11-14T10:45:20+09:00",
    "duration_seconds": 870,
    "total_jobs": 8,
    "completed_jobs": 8,
    "failed_jobs": 0,
    "success_rate": 100.0,
    "job_details": [
      {
        "job_name": "extract-job",
        "status": "SUCCEEDED",
        "duration_seconds": 120
      },
      {
        "job_name": "transform-job",
        "status": "SUCCEEDED",
        "duration_seconds": 300
      }
    ]
  },
  "summary": {
    "total_files": 48,
    "successful": 48,
    "failed": 0,
    "skipped": 0,
    "total_size_bytes": 327974912,
    "total_size_mb": 312.8,
    "duration_seconds": 25.3,
    "average_speed_mbps": 12.4,
    "success_rate": 100.0
  },
  "layers": [
    {
      "name": "source",
      "display_name": "ソース層（生データ）",
      "s3_bucket": "my-datalake-bronze",
      "s3_prefix": "raw/sales/",
      "file_count": 12,
      "min_files": 1,
      "max_files": null,
      "total_size_mb": 78.5,
      "status": "success",
      "files": [
        {
          "filename": "sales_20241101.csv",
          "s3_uri": "s3://my-datalake-bronze/raw/sales/sales_20241101.csv",
          "local_path": "downloads/layers/source/sales_20241101.csv",
          "size_mb": 6.5,
          "last_modified": "2024-11-01T08:15:30Z",
          "download_status": "success"
        },
        {
          "filename": "sales_20241102.csv",
          "s3_uri": "s3://my-datalake-bronze/raw/sales/sales_20241102.csv",
          "local_path": "downloads/layers/source/sales_20241102.csv",
          "size_mb": 6.6,
          "last_modified": "2024-11-02T08:15:30Z",
          "download_status": "success"
        }
      ]
    }
  ],
  "failed_files": [],
  "warnings": []
}
```

-----

## 11. 依存ライブラリ

### 11.1 requirements.txt

```
# AWS SDK
boto3>=1.34.0
botocore>=1.34.0

# 設定管理
PyYAML>=6.0.1
pydantic>=2.5.0

# CLI
click>=8.1.7
rich>=13.7.0  # 進捗表示・テーブル表示用

# ユーティリティ
python-dateutil>=2.8.2
tqdm>=4.66.0  # 進捗バー

# ロギング
colorlog>=6.8.0

# 開発・テスト用
pytest>=7.4.3
pytest-cov>=4.1.0
pytest-mock>=3.12.0
moto>=4.2.0  # AWS サービスのモック
black>=23.12.0
flake8>=6.1.0
mypy>=1.7.0
```

-----

## 12. セキュリティ考慮事項

### 12.1 認証・認可

|項目     |対策                         |
|-------|---------------------------|
|AWS認証  |IAMロール、プロファイル、環境変数による認証    |
|S3アクセス |最小権限の原則（読み取り専用権限）          |
|認証情報の保護|コード内にハードコードしない、.gitignore設定|

### 12.2 必要なIAMポリシー

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "glue:GetWorkflow",
        "glue:GetWorkflowRun",
        "glue:GetWorkflowRuns",
        "glue:StartWorkflowRun",
        "glue:GetJobRun",
        "glue:GetJobRuns"
      ],
      "Resource": [
        "arn:aws:glue:ap-northeast-1:123456789012:workflow/*",
        "arn:aws:glue:ap-northeast-1:123456789012:job/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:ListBucket",
        "s3:GetObject",
        "s3:GetObjectVersion"
      ],
      "Resource": [
        "arn:aws:s3:::my-datalake-bronze",
        "arn:aws:s3:::my-datalake-bronze/*",
        "arn:aws:s3:::my-datalake-silver",
        "arn:aws:s3:::my-datalake-silver/*",
        "arn:aws:s3:::my-datalake-gold",
        "arn:aws:s3:::my-datalake-gold/*"
      ]
    }
  ]
}
```

### 12.3 データ保護

|項目        |対策                    |
|----------|----------------------|
|転送中の暗号化   |HTTPS/TLS通信           |
|ローカルファイル保護|適切なファイルパーミッション設定      |
|ログの機密情報   |S3 URI以外の機密情報をログに出力しない|

-----

## 13. パフォーマンス最適化

### 13.1 最適化戦略

|項目        |対策                  |効果      |
|----------|--------------------|--------|
|並列ダウンロード  |ThreadPoolExecutor使用|最大10倍高速化|
|正規表現キャッシュ |コンパイル済みパターンのキャッシュ   |CPU使用率削減|
|条件付きダウンロード|既存ファイルのスキップ         |不要な転送削減 |
|ストリーミング   |チャンクごとのダウンロード       |メモリ使用量削減|

### 13.2 推奨設定

|ファイルサイズ    |max_workers|説明           |
|-----------|-----------|-------------|
|小（< 10MB）  |10-20      |多数の小ファイルを並列処理|
|中（10-100MB）|5-10       |バランス型        |
|大（> 100MB） |2-5        |ネットワーク帯域を考慮  |

-----

## 14. テスト戦略

### 14.1 テストレベル

|テストレベル|カバレッジ    |ツール          |
|------|---------|-------------|
|単体テスト |各クラス・メソッド|pytest       |
|統合テスト |AWS連携部分  |pytest + moto|
|E2Eテスト|全体フロー    |pytest       |

### 14.2 主要テストケース

#### 14.2.1 設定ファイルテスト

- 正常な設定ファイルの読み込み
- 必須項目の欠落検出
- 不正なYAML形式の検出
- 正規表現パターンの妥当性検証

#### 14.2.2 Workflow検証テスト

- 存在するWorkflowの検証成功
- 存在しないWorkflowの検証失敗
- 初期層ファイルの存在確認
- タイムアウト処理

#### 14.2.3 ファイル収集テスト

- 正規表現によるファイルフィルタリング
- 複数層からのファイル収集
- 空のバケット/プレフィックスの処理

#### 14.2.4 ダウンロードテスト

- 単一ファイルのダウンロード
- 並列ダウンロード
- リトライ処理
- 既存ファイルのスキップ

### 14.3 テスト実行

```bash
# 全テスト実行
pytest

# カバレッジ付き実行
pytest --cov=glue_workflow_downloader --cov-report=html

# 特定のテストのみ実行
pytest tests/test_config_manager.py

# マーカー指定実行
pytest -m unit  # 単体テストのみ
pytest -m integration  # 統合テストのみ
```

-----

## 15. 運用・保守

### 15.1 運用フロー

```
[定期実行]
  │
  ├─[1. ログ確認]
  │   └─ エラー/警告の有無確認
  │
  ├─[2. レポート確認]
  │   └─ ダウンロード成功率確認
  │
  ├─[3. ディスク容量確認]
  │   └─ ダウンロード先の容量確認
  │
  └─[4. 古いファイル削除]
      └─ 保持期間を超えたファイルの削除
```

### 15.2 監視項目

|項目       |閾値     |アクション    |
|---------|-------|---------|
|ダウンロード成功率|< 95%  |調査・対応    |
|処理時間     |> 通常の2倍|パフォーマンス調査|
|エラー発生数   |> 5件/実行|原因調査     |
|ディスク使用率  |> 80%  |古いファイル削除 |

### 15.3 トラブルシューティング

#### よくある問題と対処法

|問題         |原因             |対処法            |
|-----------|---------------|---------------|
|Workflow未検出|Workflow名の誤り   |正しいWorkflow名を確認|
|初期層ファイル未検出 |ファイルがまだ生成されていない|タイムアウト値を増加     |
|S3アクセスエラー  |IAM権限不足        |必要な権限を付与       |
|ダウンロード速度低下 |ネットワーク問題       |max_workersを調整 |
|メモリ不足      |大量ファイル処理       |max_workersを削減 |

-----

## 16. 拡張性

### 16.1 将来的な拡張案

1. **通知機能**
- 処理完了時のSlack/Email通知
- エラー発生時のアラート
1. **スケジューリング機能**
- 定期実行設定
- cron連携
1. **フィルタ機能拡張**
- 日付範囲フィルタ
- ファイルサイズフィルタ
- メタデータフィルタ
1. **圧縮機能**
- ダウンロード後の自動圧縮
- 圧縮形式の選択
1. **差分同期**
- 増分ダウンロード
- rsync的な機能
1. **マルチリージョン対応**
- 複数リージョンからのダウンロード
- レプリケーション考慮

-----

## 17. 使用例

### 17.1 基本的な使用例

```bash
# 1. 設定ファイルを作成
cat > config.yaml << EOF
version: "1.0"
aws:
  region: ap-northeast-1
workflow:
  name: "my-etl-workflow"
layers:
  - name: source
    s3_bucket: "my-bucket"
    s3_prefix: "raw/"
    file_pattern: "^data_\\d{8}\\.csv$"
    required: true
EOF

# 2. ダウンロード実行
python -m glue_workflow_downloader \
    --config config.yaml \
    --workflow my-etl-workflow
```

### 17.2 本番環境での使用例

```bash
#!/bin/bash
# daily_download.sh

LOG_DIR="/var/log/glue-downloader"
OUTPUT_DIR="/data/downloads"
DATE=$(date +%Y%m%d)

# ログディレクトリ作成
mkdir -p $LOG_DIR

# ダウンロード実行
python -m glue_workflow_downloader \
    --config /etc/glue-downloader/config.yaml \
    --workflow daily-etl-workflow \
    --output "$OUTPUT_DIR/$DATE" \
    --log-level INFO \
    2>&1 | tee "$LOG_DIR/download_$DATE.log"

# 終了コード確認
if [ $? -eq 0 ]; then
    echo "ダウンロード成功: $DATE"
    # Slack通知など
else
    echo "ダウンロード失敗: $DATE"
    # エラー通知
    exit 1
fi

# 古いファイルを削除（30日以上前）
find $OUTPUT_DIR -type d -mtime +30 -exec rm -rf {} \;
```

-----

## 18. まとめ

本設計書では、AWS Glue Workflowを実行し、管理されるETLパイプラインの複数ファイルを効率的にダウンロードするPythonツールの詳細な設計を提供しました。

### 18.1 主要な特徴

- **Workflow実行制御**: Workflowの自動実行、実行状態の監視、完了待機
- **複数ファイル対応**: 各層で正規表現による複数ファイルの一括処理
- **複数パターン対応**: 1つの層で複数の正規表現パターンを指定可能
- **ファイル数検証**: min_files/max_filesによる柔軟なファイル数制御
- **柔軟性**: YAML設定による柔軟な構成管理
- **信頼性**: 初期層確認、Workflow完了待機、リトライ処理、詳細なエラーハンドリング
- **パフォーマンス**: 並列ダウンロードによる高速化（複数ファイル対応）
- **保守性**: モジュール化された設計、詳細なログ出力
- **拡張性**: 将来の機能追加を考慮した設計

### 18.2 新規追加機能

1. **Workflow実行機能**
- Workflowの実行トリガー
- 実行状態のリアルタイム監視
- ジョブ完了待機とタイムアウト処理
1. **複数ファイル対応**
- 正規表現による複数ファイルの一括指定
- ファイル数の検証（最小/最大）
- 層ごとの複数ファイル管理
1. **★ 複数パターン対応（最新追加）**
- 1つの層で複数の正規表現パターンを配列で指定
- 異なる形式・命名規則のファイルを同一層で管理
- パターンごとのマッチング結果追跡
- 例: CSV、JSON、Parquetを同一層で扱う
1. **拡張されたレポート**
- Workflow実行結果の詳細
- 各層の複数ファイル情報
- マッチしたパターン情報
- ジョブ別の実行時間

### 18.3 複数パターン対応のユースケース

- **異なるファイル形式の混在**: CSV、JSON、Parquetを同一層で管理
- **異なる粒度のデータ**: 日次、週次、月次、年次データの同時収集
- **複数データソース**: 顧客、商品、注文データの統合管理
- **バージョン管理**: 複数バージョンのファイルの一括ダウンロード
- **パーティション構造**: 異なるパーティションキーのファイル対応

### 18.4 次のステップ

1. 本設計書に基づいた実装
1. 単体テスト・統合テストの作成
1. 本番環境でのパイロット運用
1. フィードバックに基づく改善

-----

## 付録A: 設定ファイルサンプル集

### A.1 シンプルな構成

```yaml
version: "1.0"
aws:
  region: ap-northeast-1
workflow:
  name: "simple-etl"
layers:
  - name: input
    s3_bucket: "data-lake"
    s3_prefix: "input/"
    file_patterns:  # 複数パターン対応
      - ".*\\.csv$"
      - ".*\\.json$"
    required: true
  - name: output
    s3_bucket: "data-lake"
    s3_prefix: "output/"
    file_patterns:
      - ".*\\.parquet$"
download:
  local_base_dir: "./downloads"
```

### A.2 複雑なマルチ層構成（複数パターン対応）

```yaml
version: "1.0"
aws:
  region: ap-northeast-1
  profile: production
workflow:
  name: "enterprise-etl-pipeline"
  execute: true
  validate_before_run: true
  initial_layer_check_timeout: 600
  execution_timeout: 3600
  polling_interval: 30
layers:
  # 生データ層 - 複数形式・複数パターン対応
  - name: raw
    display_name: "生データ層"
    s3_bucket: "datalake-bronze"
    s3_prefix: "raw/transactions/"
    file_patterns:
      - "^transaction_\\d{8}_\\d{6}\\.csv$"      # CSV形式の日次データ
      - "^transaction_stream_\\d{8}\\.json$"     # JSONストリーミングデータ
      - "^transaction_batch_\\d{6}\\.csv$"       # 月次バッチデータ
    required: true
    min_files: 1
    max_files: null
    
  # クレンジング層 - 複数形式対応
  - name: cleansed
    display_name: "クレンジング層"
    s3_bucket: "datalake-silver"
    s3_prefix: "cleansed/transactions/"
    file_patterns:
      - "^transaction_cleansed_\\d{8}\\.parquet$"
      - "^transaction_validated_\\d{8}\\.parquet$"
    min_files: 0
    
  # エンリッチメント層 - 複数データソース
  - name: enriched
    display_name: "エンリッチメント層"
    s3_bucket: "datalake-silver"
    s3_prefix: "enriched/transactions/"
    file_patterns:
      - "^transaction_enriched_\\d{8}\\.parquet$"
      - "^customer_enriched_\\d{8}\\.parquet$"
      - "^product_enriched_\\d{8}\\.parquet$"
    
  # 集計層 - 複数集計レベル
  - name: aggregated
    display_name: "集計層"
    s3_bucket: "datalake-gold"
    s3_prefix: "aggregated/"
    file_patterns:
      - "^daily/summary_\\d{8}\\.parquet$"       # 日次集計
      - "^weekly/summary_\\d{6}_W\\d{2}\\.parquet$"  # 週次集計
      - "^monthly/summary_\\d{6}\\.parquet$"     # 月次集計
    min_files: 1
    
  # データマート層 - 複数マート・複数形式
  - name: mart
    display_name: "データマート層"
    s3_bucket: "datalake-gold"
    s3_prefix: "mart/"
    file_patterns:
      - "^sales/sales_mart_\\d{8}\\.(parquet|csv)$"      # 売上マート
      - "^customer/customer_mart_\\d{8}\\.(parquet|csv)$"  # 顧客マート
      - "^inventory/inventory_mart_\\d{8}\\.(parquet|csv)$"  # 在庫マート
      - "^financial/financial_report_\\d{4}Q[1-4]\\.xlsx$"  # 財務レポート
    min_files: 1
    max_files: 200
    
download:
  local_base_dir: "/data/etl-downloads"
  preserve_structure: true
  overwrite: false
  max_workers: 10
  retry_count: 5
  retry_delay: 10
  
logging:
  level: INFO
  file: "/var/log/glue-downloader/app.log"
  console: true
  
filters:
  date_range:
    enabled: true
    start_date: "2024-11-01"
    end_date: "2024-11-14"
```

-----

## 付録B: 正規表現パターン例

### B.1 単一パターンの例

```yaml
# CSVファイル（日付付き）
file_patterns:
  - "^data_\\d{8}\\.csv$"
# 例: data_20241114.csv

# Parquetファイル（日付+時刻付き）
file_patterns:
  - "^result_\\d{8}_\\d{6}\\.parquet$"
# 例: result_20241114_103045.parquet

# 複数拡張子対応
file_patterns:
  - "^output_\\d{8}\\.(csv|parquet|json)$"
# 例: output_20241114.csv, output_20241114.parquet

# パーティション対応
file_patterns:
  - "^year=\\d{4}/month=\\d{2}/day=\\d{2}/.*\\.parquet$"
# 例: year=2024/month=11/day=14/data.parquet

# プレフィックス+UUID
file_patterns:
  - "^transaction_[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\\.csv$"
# 例: transaction_550e8400-e29b-41d4-a716-446655440000.csv
```

### B.2 複数パターンの実践例

#### 例1: 異なる形式のファイルを混在させる

```yaml
file_patterns:
  - "^sales_daily_\\d{8}\\.csv$"      # 日次売上データ
  - "^sales_weekly_\\d{6}_W\\d{2}\\.csv$"  # 週次集計
  - "^sales_monthly_\\d{6}\\.csv$"    # 月次集計
  - "^sales_yearly_\\d{4}\\.csv$"     # 年次集計
# マッチ例:
# - sales_daily_20241114.csv
# - sales_weekly_202411_W02.csv
# - sales_monthly_202411.csv
# - sales_yearly_2024.csv
```

#### 例2: 異なる拡張子のファイル

```yaml
file_patterns:
  - "^report_\\d{8}\\.parquet$"       # Parquet形式
  - "^report_\\d{8}\\.csv$"           # CSV形式
  - "^report_\\d{8}\\.json$"          # JSON形式
  - "^report_summary_\\d{8}\\.pdf$"   # PDFレポート
# マッチ例:
# - report_20241114.parquet
# - report_20241114.csv
# - report_20241114.json
# - report_summary_20241114.pdf
```

#### 例3: 異なるプレフィックスのファイル

```yaml
file_patterns:
  - "^customer_\\d{8}\\.parquet$"     # 顧客データ
  - "^product_\\d{8}\\.parquet$"      # 商品データ
  - "^order_\\d{8}\\.parquet$"        # 注文データ
  - "^inventory_\\d{8}\\.parquet$"    # 在庫データ
# マッチ例:
# - customer_20241114.parquet
# - product_20241114.parquet
# - order_20241114.parquet
# - inventory_20241114.parquet
```

#### 例4: パーティション構造の複数パターン

```yaml
file_patterns:
  - "^year=\\d{4}/month=\\d{2}/day=\\d{2}/.*\\.parquet$"     # 日次パーティション
  - "^year=\\d{4}/month=\\d{2}/.*\\.parquet$"                # 月次パーティション
  - "^region=[A-Z]{2}/year=\\d{4}/.*\\.parquet$"             # リージョン別パーティション
# マッチ例:
# - year=2024/month=11/day=14/data.parquet
# - year=2024/month=11/summary.parquet
# - region=US/year=2024/data.parquet
```

#### 例5: バージョン管理されたファイル

```yaml
file_patterns:
  - "^data_v\\d+\\.\\d+\\.\\d+_\\d{8}\\.parquet$"  # セマンティックバージョン
  - "^data_\\d{8}_v\\d+\\.parquet$"                # シンプルバージョン
  - "^data_\\d{8}_final\\.parquet$"                # 最終版
  - "^data_\\d{8}_draft\\.parquet$"                # ドラフト版
# マッチ例:
# - data_v1.2.3_20241114.parquet
# - data_20241114_v5.parquet
# - data_20241114_final.parquet
# - data_20241114_draft.parquet
```

#### 例6: 複雑な命名規則

```yaml
file_patterns:
  - "^[A-Z]{3}_[A-Z]{2}_\\d{8}_\\d{6}\\.csv$"              # コード+日時
  - "^[a-z]+_[0-9a-f]{8}\\.json$"                          # 名前+ハッシュ
  - "^\\d{4}Q[1-4]_financial_report\\.(xlsx|csv)$"         # 四半期レポート
  - "^backup_\\d{8}_\\d{6}_[0-9a-f]{32}\\.tar\\.gz$"       # バックアップ
# マッチ例:
# - USD_US_20241114_103045.csv
# - customer_a1b2c3d4.json
# - 2024Q3_financial_report.xlsx
# - backup_20241114_103045_d41d8cd98f00b204e9800998ecf8427e.tar.gz
```

### B.3 使用上の注意点

#### パターンの順序

- パターンは記述順に評価されます
- より具体的なパターンを先に記述することを推奨

#### パフォーマンス考慮

- パターン数が多い場合、マッチング時間が増加する可能性があります
- 可能な限り効率的な正規表現を使用してください

#### デバッグ

- 複雑なパターンは事前にテストツール（regex101.comなど）で検証することを推奨
- 意図しないファイルがマッチしないよう、`^`（行頭）と`$`（行末）を使用してください

-----

## 付録C: エラーコード一覧

|コード |説明            |終了コード|
|----|--------------|-----|
|E001|設定ファイル読み込みエラー |1    |
|E002|設定検証エラー       |1    |
|E003|Workflow未検出   |2    |
|E004|初期層ファイル未検出    |3    |
|E005|S3アクセスエラー     |4    |
|E006|ダウンロードエラー     |5    |
|E007|ディスク容量不足      |6    |
|E008|権限エラー         |7    |
|W001|ファイルスキップ（警告）  |0    |
|W002|一部ダウンロード失敗（警告）|0    |

-----

**文書バージョン**: 1.0  
**作成日**: 2024-11-14  
**最終更新日**: 2024-11-14  
**作成者**: Furuhata
